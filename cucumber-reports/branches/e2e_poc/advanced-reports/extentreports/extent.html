









<!DOCTYPE html>
<html>
<head>
	<meta charset='UTF-8' /> 
	<meta name='description' content='' />
	<meta name='robots' content='noodp, noydir' />
	<meta name='viewport' content='width=device-width, initial-scale=1' />
	<meta id="timeStampFormat" name="timeStampFormat" content='MMM d, yyyy hh:mm:ss a'/>
	
        <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600' rel='stylesheet' type='text/css' />
        <link href="http://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet" />
        <link href='http://cdn.jsdelivr.net/gh/extent-framework/extent-github-cdn@ff53917fbbdb5ef820abbbe4d199a6942dc771ff/v3html/css/extent.css' type='text/css' rel='stylesheet' />
	
	<title>Extent</title>

	<style type='text/css'>
		/* json-tree */
		.jstBracket,.jstComma,.jstValue{white-space:pre-wrap}.jstValue{font-size:10px;font-weight:400;font-family:"Lucida Console",Monaco,monospace}.jstProperty{color:#666;word-wrap:break-word}.jstBool{color:#2525CC}.jstNum{color:#D036D0}.jstNull{color:gray}.jstStr{color:#2DB669}.jstFold:after{content:' -';cursor:pointer}.jstExpand{white-space:normal}.jstExpand:after{content:' +';cursor:pointer}.jstFolded{white-space:normal!important}.jstHiddenBlock{display:none}
			


		
	</style>
	
	<script type="text/javascript">
		/*! json-tree - v0.2.2 - 2017-09-25, MIT LICENSE */
		var JSONTree=function(){var n={"&":"&amp;","<":"&lt;",">":"&gt;",'"':"&quot;","'":"&#x27;","/":"&#x2F;"},t=0,r=0;this.create=function(n,t){return r+=1,N(u(n,0,!1),{class:"jstValue"})};var e=function(t){return t.replace(/[&<>'"]/g,function(t){return n[t]})},s=function(){return r+"_"+t++},u=function(n,t,r){if(null===n)return f(r?t:0);switch(typeof n){case"boolean":return l(n,r?t:0);case"number":return i(n,r?t:0);case"string":return o(n,r?t:0);default:return n instanceof Array?a(n,t,r):c(n,t,r)}},c=function(n,t,r){var e=s(),u=Object.keys(n).map(function(r){return j(r,n[r],t+1,!0)}).join(m()),c=[g("{",r?t:0,e),N(u,{id:e}),p("}",t)].join("\n");return N(c,{})},a=function(n,t,r){var e=s(),c=n.map(function(n){return u(n,t+1,!0)}).join(m());return[g("[",r?t:0,e),N(c,{id:e}),p("]",t)].join("\n")},o=function(n,t){var r=e(JSON.stringify(n));return N(v(r,t),{class:"jstStr"})},i=function(n,t){return N(v(n,t),{class:"jstNum"})},l=function(n,t){return N(v(n,t),{class:"jstBool"})},f=function(n){return N(v("null",n),{class:"jstNull"})},j=function(n,t,r){var s=v(e(JSON.stringify(n))+": ",r),c=N(u(t,r,!1),{});return N(s+c,{class:"jstProperty"})},m=function(){return N(",\n",{class:"jstComma"})},N=function(n,t){return d("span",t,n)},d=function(n,t,r){return"<"+n+Object.keys(t).map(function(n){return" "+n+'="'+t[n]+'"'}).join("")+">"+r+"</"+n+">"},g=function(n,t,r){return N(v(n,t),{class:"jstBracket"})+N("",{class:"jstFold",onclick:"JSONTree.toggle('"+r+"')"})};this.toggle=function(n){var t=document.getElementById(n),r=t.parentNode,e=t.previousElementSibling;""===t.className?(t.className="jstHiddenBlock",r.className="jstFolded",e.className="jstExpand"):(t.className="",r.className="",e.className="jstFold")};var p=function(n,t){return N(v(n,t),{})},v=function(n,t){return Array(2*t+1).join(" ")+n};return this}();
	</script>
</head>
	<body class='extent standard default hide-overflow bdd-report'>
		<div id='theme-selector' alt='Click to toggle theme. To enable by default, use theme configuration.' title='Click to toggle theme. To enable by default, use theme configuration.'>
			<span><i class='material-icons'>desktop_windows</i></span>
		</div>
<nav>
	<div class="nav-wrapper">
		<a href="#!" class="brand-logo black"><img src="https://cdn.rawgit.com/extent-framework/extent-github-cdn/d74480e/commons/img/logo.png"></a>
		<!-- slideout menu -->
		<ul id='slide-out' class='side-nav fixed hide-on-med-and-down'>
			<li class='waves-effect active'><a href='#!' view='test-view' onclick="configureView(0);chartsView('test');"><i class='material-icons'>dashboard</i></a></li>
						<li class='waves-effect'><a href='#!' view='category-view' onclick="configureView(1)"><i class='material-icons'>label_outline</i></a></li>
			<li class='waves-effect'><a href='#!' view='exception-view' onclick="configureView(2)"><i class='material-icons'>bug_report</i></a></li>
			<li class='waves-effect'><a href='#!' onclick="configureView(-1);chartsView('dashboard');" view='dashboard-view'><i class='material-icons'>track_changes</i></a></li>
		</ul>
		<!-- report name -->
		<span class='report-name'>Sahil Extent Report</span>
		<!-- report headline -->
		<span class='report-headline'></span>
		<!-- nav-right -->
		<ul id='nav-mobile' class='right hide-on-med-and-down nav-right'>
			<a href='#!'>
			<span class='label blue darken-3 suite-start-time'>Oct 1, 2021 05:42:48 PM</span>
			</a>
		</ul>
	</div>
</nav>		<!-- container -->
		<div class='container'>
<div id='test-view' class='view'>
	<section id='controls'>
		<div class='controls grey lighten-4'>
			<!-- test toggle -->
			<div class='chip transparent'>
				<a class='dropdown-button tests-toggle' data-activates='tests-toggle' data-constrainwidth='true' data-beloworigin='true' data-hover='true' href='#'>
				<i class='material-icons'>warning</i> Status
				</a>
				<ul id='tests-toggle' class='dropdown-content'>
										<li status='pass'><a href='#!'>Pass <i class='material-icons green-text'>check_circle</i></a></li>
					<li status='fail'><a href='#!'>Fail <i class='material-icons red-text'>cancel</i></a></li>
					<li status='skip'><a href='#!'>Skip <i class='material-icons cyan-text'>redo</i></a></li>
					<li class='divider'></li>
					<li status='clear' clear='true'><a href='#!'>Clear Filters <i class='material-icons'>clear</i></a></li>
				</ul>
			</div>
			<!-- test toggle -->
			<!-- category toggle -->
			<div class='chip transparent'>
				<a class='dropdown-button category-toggle' data-activates='category-toggle' data-constrainwidth='false' data-beloworigin='true' data-hover='true' href='#'>
				<i class='material-icons'>local_offer</i> Category
				</a>
				<ul id='category-toggle' class='dropdown-content'>
					<li><a href='#'>@TC-Demo-1</a></li>
					<li class='divider'></li>
					<li class='clear'><a href='#!' clear='true'>Clear Filters</a></li>
				</ul>
			</div>
			<!-- category toggle -->
			<!-- clear filters -->
			<div class='chip transparent hide'>
				<a class='' id='clear-filters' alt='Clear Filters' title='Clear Filters'>
				<i class='material-icons'>close</i> Clear
				</a>
			</div>
			<!-- clear filters -->
			<!-- enable dashboard -->
			<div id='toggle-test-view-charts' class='chip transparent'>
				<a class='pink-text' id='enable-dashboard' alt='Enable Dashboard' title='Enable Dashboard'>
				<i class='material-icons'>track_changes</i> Dashboard
				</a>
			</div>
			<!-- enable dashboard -->
			<!-- search -->
			<div class='chip transparent' alt='Search Tests' title='Search Tests'>
				<a href="#" class='search-div'>
				<i class='material-icons'>search</i> Search
				</a>
				<div class='input-field left hide'>
					<input id='search-tests' type='text' class='validate browser-default' placeholder='Search Tests...'>
				</div>
			</div>
			<!-- search -->
		</div>
	</section>
<div id='test-view-charts' class='subview-full'>
    <div id='charts-row' class='row nm-v nm-h'>
        <div class='col s12 m4 l4 np-h'>
            <div class='card-panel nm-v'>
                <div class='left panel-name'>Features</div>
                <div class='chart-box' style="max-height:94px;">
                    <canvas id='parent-analysis' width='90' height='70'></canvas>
                </div>
                <div class='block text-small'>
                    <span class='tooltipped' data-position='top' data-tooltip='0%'><span class='strong'>0</span> feature(s) passed</span>
                </div>
                <div class='block text-small'>
                    <span class='strong tooltipped' data-position='top' data-tooltip='100%'>1</span> feature(s) failed, <span class='strong tooltipped' data-position='top' data-tooltip='0%'>0</span> skipped
                </div>
            </div>
        </div>
        <div class='col s12 m4 l4 np-h'>
            <div class='card-panel nm-v'>
                <div class='left panel-name'>Scenarios</div>
                <div class='chart-box' style="max-height:94px;">
                    <canvas id='child-analysis' width='90' height='70'></canvas>
                </div>
                <div class='block text-small'>
                    <span class='tooltipped' data-position='top' data-tooltip='0%'><span class='strong'>0</span> scenario(s) passed</span>
                </div>
                <div class='block text-small'>
                    <span class='strong tooltipped' data-position='top' data-tooltip='100%'>1</span> scenario(s) failed, 
                    <span class='strong tooltipped' data-position='top' data-tooltip='0%'>0</span> skipped, 
                    <span class='strong tooltipped' data-position='top' data-tooltip='0%'>0</span> others
                </div>
            </div>
        </div>
        <div class='col s12 m4 l4 np-h'>
            <div class='card-panel nm-v'>
                <div class='left panel-name'>Steps</div>
                <div class='chart-box' style="max-height:94px;">
                    <canvas id='grandchild-analysis' width='90' height='70'></canvas>
                </div>
                <div class='block text-small'>
                    <span class='tooltipped' data-position='top' data-tooltip='78.125%'><span class='strong'>25</span> step(s) passed</span>
                </div>
                <div class='block text-small'>
                    <span class='strong tooltipped' data-position='top' data-tooltip='3.125%'>1</span> scenario(s) failed, 
                    <span class='strong tooltipped' data-position='top' data-tooltip='18.75%'>6</span> skipped, 
                    <span class='strong tooltipped' data-position='top' data-tooltip='18.75%'>0</span> others
                </div>
            </div>
        </div>
    </div>
    <div id="timeline-chart" class="row nm-v nm-h">
        <div class="col s12 m12 l12 np-h">
            <div class="card-panel">
                <div class='left panel-name'>Timeline (seconds)</div>
                <div class="chart-box" style="width:98%;max-height:145px;">
                    <canvas id="timeline" height="120"></canvas>
                </div>
            </div>
        </div>
    </div>
</div>	<div class='subview-left left'>
		<div class='view-summary'>
			<ul id='test-collection' class='test-collection'>
				<li class='test displayed active has-leaf fail' status='fail' bdd='true' test-id='1'>
					<div class='test-heading'>
						<span class='test-name'>Demo1</span>
						<span class='test-time'>Oct 1, 2021 05:42:49 PM</span>
						<span class='test-status right fail'>fail</span>
					</div>
					<div class='test-content hide'>
<div class="sr-filters bdd-filters">
	<a class="btn-floating waves-effect waves-light pass green" title="pass"><i class='material-icons'>check_circle</i></a>
	<a class="btn-floating waves-effect waves-light fail red" title="fail"><i class='material-icons'>cancel</i></a>
	<a class="btn-floating waves-effect waves-light skip blue" title="skip"><i class='material-icons'>redo</i></a>
	<a class="btn-floating waves-effect waves-light clear grey" title="clear"><i class='material-icons'>clear</i></a>
</div>
<div class='scenario node' test-id='2' status='fail'>
	<div class='category-list'>
		<span class='category label'>@TC-Demo-1</span>
	</div>
	<span class='duration right label'>0h 2m 28s+420ms</span>
	<div class="bdd-test">
		<div class="scenario-name"><span class='status fail' title='fail'><i class='material-icons'>cancel</i></span> Scenario: User is able to Login and confirm data is getting transferred from GCS to BigQuery</div>
	</div>
	<ul class='steps'>
		<li test-id='4' class='node given pass' status='pass'>
			<div class="step-name" title="GCSBasicDemo.openDatafusionProjectToConfigurePipeline()"><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>Given Open Datafusion Project to configure pipeline</div>
			<div class="node-step"></div>
		</li>
		<li test-id='5' class='node * pass' status='pass'>
			<div class="step-name" title=""><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>AfterActions.tearDown(Scenario)</div>
			<div class="node-step"></div>
		</li>
		<li test-id='6' class='node when pass' status='pass'>
			<div class="step-name" title="GCSBasicDemo.sourceIsGCSBucket()"><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>When Source is GCS bucket</div>
			<div class="node-step"></div>
		</li>
		<li test-id='7' class='node * pass' status='pass'>
			<div class="step-name" title=""><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>AfterActions.tearDown(Scenario)</div>
			<div class="node-step"></div>
		</li>
		<li test-id='8' class='node when pass' status='pass'>
			<div class="step-name" title="GCSBasicDemo.targetIsBigQuery()"><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>When Target is BigQuery</div>
			<div class="node-step"></div>
		</li>
		<li test-id='9' class='node * pass' status='pass'>
			<div class="step-name" title=""><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>AfterActions.tearDown(Scenario)</div>
			<div class="node-step"></div>
		</li>
		<li test-id='10' class='node then pass' status='pass'>
			<div class="step-name" title="GCSBasicDemo.linkSourceAndSinkToEstablishConnection()"><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>Then Link Source and Sink to establish connection</div>
			<div class="node-step"></div>
		</li>
		<li test-id='11' class='node * pass' status='pass'>
			<div class="step-name" title=""><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>AfterActions.tearDown(Scenario)</div>
			<div class="node-step"></div>
		</li>
		<li test-id='12' class='node then pass' status='pass'>
			<div class="step-name" title="GCSBasicDemo.enterTheGCSPropertiesWithGCSBucket(String)"><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>Then Enter the GCS Properties with "@TC-Demo-1_GCS" GCS bucket</div>
			<div class="node-step"></div>
		</li>
		<li test-id='13' class='node * pass' status='pass'>
			<div class="step-name" title=""><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>AfterActions.tearDown(Scenario)</div>
			<div class="node-step"></div>
		</li>
		<li test-id='14' class='node then pass' status='pass'>
			<div class="step-name" title="GCSBasicDemo.closeTheGCSProperties()"><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>Then Close the GCS Properties</div>
			<div class="node-step"></div>
		</li>
		<li test-id='15' class='node * pass' status='pass'>
			<div class="step-name" title=""><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>AfterActions.tearDown(Scenario)</div>
			<div class="node-step"></div>
		</li>
		<li test-id='16' class='node then pass' status='pass'>
			<div class="step-name" title="GCSBasicDemo.enterTheBigQueryPropertiesForTable(String)"><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>Then Enter the BigQuery Properties for table "tableDemo"</div>
			<div class="node-step"></div>
		</li>
		<li test-id='17' class='node * pass' status='pass'>
			<div class="step-name" title=""><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>AfterActions.tearDown(Scenario)</div>
			<div class="node-step"></div>
		</li>
		<li test-id='18' class='node then pass' status='pass'>
			<div class="step-name" title="GCSBasicDemo.closeTheBigQueryProperties()"><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>Then Close the BigQuery Properties</div>
			<div class="node-step"></div>
		</li>
		<li test-id='19' class='node * pass' status='pass'>
			<div class="step-name" title=""><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>AfterActions.tearDown(Scenario)</div>
			<div class="node-step"></div>
		</li>
		<li test-id='20' class='node then pass' status='pass'>
			<div class="step-name" title="GCSBasicDemo.saveAndDeployPipeline()"><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>Then Save and Deploy Pipeline</div>
			<div class="node-step"></div>
		</li>
		<li test-id='21' class='node * pass' status='pass'>
			<div class="step-name" title=""><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>AfterActions.tearDown(Scenario)</div>
			<div class="node-step"></div>
		</li>
		<li test-id='22' class='node then pass' status='pass'>
			<div class="step-name" title="GCSBasicDemo.runThePipelineInRuntime()"><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>Then Run the Pipeline in Runtime</div>
			<div class="node-step"></div>
		</li>
		<li test-id='23' class='node * pass' status='pass'>
			<div class="step-name" title=""><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>AfterActions.tearDown(Scenario)</div>
			<div class="node-step"></div>
		</li>
		<li test-id='24' class='node then pass' status='pass'>
			<div class="step-name" title="GCSBasicDemo.waitTillPipelineIsInRunningState()"><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>Then Wait till pipeline is in running state</div>
			<div class="node-step"></div>
		</li>
		<li test-id='25' class='node * pass' status='pass'>
			<div class="step-name" title=""><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>AfterActions.tearDown(Scenario)</div>
			<div class="node-step"></div>
		</li>
		<li test-id='26' class='node then pass' status='pass'>
			<div class="step-name" title="GCSBasicDemo.openLogs()"><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>Then Open Logs</div>
			<div class="node-step">2021-10-01 17:43:36,829 - DEBUG [provisioning-task-0:i.c.c.i.p.t.ProvisioningTask@125] - Executing PROVISION subtask REQUESTING_CREATE for program run program_run:default.TestPipelinefbe63d25-c448-4ed4-baa5-22cafd7d3b66.-SNAPSHOT.workflow.DataPipelineWorkflow.1aeb91a4-22df-11ec-9a21-eee09c71e460.
2021-10-01 17:43:36,832 - DEBUG [provisioning-task-0:i.c.c.i.p.t.ProvisioningTask@129] - Completed PROVISION subtask REQUESTING_CREATE for program run program_run:default.TestPipelinefbe63d25-c448-4ed4-baa5-22cafd7d3b66.-SNAPSHOT.workflow.DataPipelineWorkflow.1aeb91a4-22df-11ec-9a21-eee09c71e460.
2021-10-01 17:43:36,854 - DEBUG [provisioning-task-0:i.c.c.i.p.t.ProvisioningTask@116] - Completed PROVISION task for program run program_run:default.TestPipelinefbe63d25-c448-4ed4-baa5-22cafd7d3b66.-SNAPSHOT.workflow.DataPipelineWorkflow.1aeb91a4-22df-11ec-9a21-eee09c71e460.
2021-10-01 17:43:40,398 - INFO  [WorkflowDriver:i.c.c.d.SmartWorkflow@470] - Pipeline 'TestPipelinefbe63d25-c448-4ed4-baa5-22cafd7d3b66' is started by user 'runner' with arguments {logical.start.time=1633110216634}
2021-10-01 17:43:40,436 - INFO  [WorkflowDriver:i.c.c.d.SmartWorkflow@505] - Pipeline 'TestPipelinefbe63d25-c448-4ed4-baa5-22cafd7d3b66' running
2021-10-01 17:43:40,441 - DEBUG [WorkflowDriver:i.c.c.i.a.r.w.WorkflowProgramController@71] - Workflow service workflow.default.TestPipelinefbe63d25-c448-4ed4-baa5-22cafd7d3b66.DataPipelineWorkflow.1aeb91a4-22df-11ec-9a21-eee09c71e460 started
2021-10-01 17:43:40,442 - INFO  [WorkflowDriver:i.c.c.i.a.r.w.WorkflowDriver@623] - Starting workflow execution for 'DataPipelineWorkflow' with Run id '1aeb91a4-22df-11ec-9a21-eee09c71e460'
2021-10-01 17:43:40,475 - INFO  [action-phase-1-0:i.c.c.i.a.r.w.WorkflowDriver@337] - Starting Spark Program 'phase-1' in workflow
2021-10-01 17:43:40,516 - DEBUG [action-phase-1-0:i.c.c.a.r.s.SparkProgramRunner@231] - Starting Spark Job. Context: SparkRuntimeContext{id=program:default.TestPipelinefbe63d25-c448-4ed4-baa5-22cafd7d3b66.-SNAPSHOT.spark.phase-1, runId=1d3581a5-22df-11ec-af7e-eee09c71e460}
2021-10-01 17:43:44,559 - DEBUG [SparkRunnerphase-1:i.c.p.g.b.s.AbstractBigQuerySink@136] - Init output for table 'DemoCheck1' with schema: {"type":"record","name":"text","fields":[{"name":"name","type":"string"},{"name":"age","type":"int"}]}
2021-10-01 17:43:44,846 - DEBUG [SparkRunnerphase-1:i.c.c.i.a.r.LocalizationUtils@55] - Hard link file from /tmp/HydratorSpark1974581448145022373.config to /tmp/google-cloud/google-cloud/sandbox/cdap-sandbox-6.6.0-SNAPSHOT/data/tmp/1633110220520-0/HydratorSpark.config
2021-10-01 17:43:44,869 - DEBUG [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:i.c.c.a.r.s.s.AbstractSparkSubmitter@164] - Calling SparkSubmit for program:default.TestPipelinefbe63d25-c448-4ed4-baa5-22cafd7d3b66.-SNAPSHOT.spark.phase-1 1d3581a5-22df-11ec-af7e-eee09c71e460: [--master, local[2], --conf, spark.app.name=phase-1, --conf, spark.executor.memory=2048m, --conf, spark.driver.memory=2048m, --conf, spark.local.dir=/tmp/google-cloud/google-cloud/sandbox/cdap-sandbox-6.6.0-SNAPSHOT/data/tmp/1633110220520-0, --conf, spark.driver.cores=1, --conf, spark.yarn.maxAppAttempts=1, --conf, spark.executor.id=TestPipelinefbe63d25-c448-4ed4-baa5-22cafd7d3b66, --conf, spark.network.timeout=600s, --conf, spark.executor.cores=1, --conf, spark.metrics.conf=/tmp/google-cloud/google-cloud/sandbox/cdap-sandbox-6.6.0-SNAPSHOT/data/tmp/1633110220520-0/metrics.properties, --conf, spark.speculation=false, --conf, spark.sql.caseSensitive=true, --conf, spark.app.id=TestPipelinefbe63d25-c448-4ed4-baa5-22cafd7d3b66, --conf, spark.maxRemoteBlockSizeFetchToMem=2147483135, --conf, spark.extraListeners=io.cdap.cdap.app.runtime.spark.DelegatingSparkListener, --conf, spark.sql.autoBroadcastJoinThreshold=-1, --conf, spark.executor.extraJavaOptions=-Dstreaming.checkpoint.rewrite.enabled=true, --conf, spark.cdap.localized.resources=["HydratorSpark.config"], --conf, spark.driver.extraJavaOptions=-Dstreaming.checkpoint.rewrite.enabled=true, --class, io.cdap.cdap.app.runtime.spark.SparkMainWrapper, /tmp/google-cloud/google-cloud/sandbox/cdap-sandbox-6.6.0-SNAPSHOT/data/runtime/spark/cdapSparkJob.jar, --cdap.spark.program=program_run:default.TestPipelinefbe63d25-c448-4ed4-baa5-22cafd7d3b66.-SNAPSHOT.spark.phase-1.1d3581a5-22df-11ec-af7e-eee09c71e460, --cdap.user.main.class=io.cdap.cdap.etl.spark.batch.BatchSparkPipelineDriver]
2021-10-01 17:43:44,994 - INFO  [SparkDriverHttpService STARTING:i.c.h.NettyHttpService@181] - Starting HTTP Service phase-1-http-service at address localhost/127.0.0.1:0
2021-10-01 17:43:44,995 - DEBUG [SparkDriverHttpService STARTING:i.c.h.NettyHttpService@191] - Started HTTP Service phase-1-http-service at address /127.0.0.1:43627
2021-10-01 17:43:45,011 - INFO  [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:i.c.c.a.r.s.SparkMainWrapper$@78] - Launching user spark class class io.cdap.cdap.etl.spark.batch.BatchSparkPipelineDriver
2021-10-01 17:43:45,103 - WARN  [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:o.a.s.SparkConf@66] - In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).
2021-10-01 17:43:46,236 - DEBUG [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:i.c.c.a.r.s.SparkMetricsSink@51] - Using SparkMetricsSink for reporting metrics: {class=io.cdap.cdap.app.runtime.spark.SparkMetricsSink}
2021-10-01 17:43:47,342 - INFO  [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:c.g.c.h.i.b.o.ForwardingBigQueryFileOutputFormat@76] - Using output path 'gs://13d3737e-a176-4b78-beab-5f18dcca2353/13d3737e-a176-4b78-beab-5f18dcca2353/input/DemoCheck1-13d3737e-a176-4b78-beab-5f18dcca2353'.
2021-10-01 17:43:47,645 - INFO  [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:c.g.c.h.i.b.BigQueryFactory@76] - Bigquery connector version hadoop2-1.0.0
2021-10-01 17:43:47,647 - INFO  [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from default credential.
2021-10-01 17:43:47,648 - INFO  [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from given credential.
2021-10-01 17:43:47,649 - INFO  [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:c.g.c.h.i.b.o.ForwardingBigQueryFileOutputFormat@76] - Delegating functionality to 'AvroOutputFormat'.
2021-10-01 17:43:47,955 - INFO  [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:c.g.c.h.i.b.o.ForwardingBigQueryFileOutputFormat@76] - Delegating functionality to 'AvroOutputFormat'.
2021-10-01 17:43:48,026 - INFO  [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from default credential.
2021-10-01 17:43:48,027 - INFO  [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from given credential.
2021-10-01 17:43:48,028 - INFO  [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from default credential.
2021-10-01 17:43:48,029 - INFO  [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from given credential.
2021-10-01 17:43:48,835 - DEBUG [SparkListenerBus:i.c.c.a.r.s.AbstractSparkExecutionContext@156] - Spark program=program:default.TestPipelinefbe63d25-c448-4ed4-baa5-22cafd7d3b66.-SNAPSHOT.spark.phase-1, runId=1d3581a5-22df-11ec-af7e-eee09c71e460, jobId=0 starts with auto-commit=false on transaction Transaction{readPointer: 1633110226293000000, transactionId: 1633110226328000000, writePointer: 1633110226328000000, invalids: [], inProgress: [1633110226294000000], firstShortInProgress: 1633110226294000000, type: LONG, checkpointWritePointers: [], visibilityLevel: SNAPSHOT}
2021-10-01 17:43:48,838 - DEBUG [SparkListenerBus:i.c.c.a.r.s.SparkTransactionHandler@110] - Spark job started: JobTransaction{jobId=0, stageIds=[0], transaction=Transaction{readPointer: 1633110226293000000, transactionId: 1633110226328000000, writePointer: 1633110226328000000, invalids: [], inProgress: [1633110226294000000], firstShortInProgress: 1633110226294000000, type: LONG, checkpointWritePointers: [], visibilityLevel: SNAPSHOT}}
2021-10-01 17:43:49,266 - INFO  [Executor task launch worker for task 0:c.g.c.h.i.b.o.ForwardingBigQueryFileOutputFormat@76] - Delegating functionality to 'AvroOutputFormat'.
2021-10-01 17:43:49,327 - INFO  [Executor task launch worker for task 0:c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from default credential.
2021-10-01 17:43:49,328 - INFO  [Executor task launch worker for task 0:c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from given credential.
2021-10-01 17:43:49,328 - INFO  [Executor task launch worker for task 0:c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from default credential.
2021-10-01 17:43:49,329 - INFO  [Executor task launch worker for task 0:c.g.c.h.i.b.BigQueryFactory@76] - Creating BigQuery from given credential.
2021-10-01 17:43:49,334 - INFO  [Executor task launch worker for task 0:c.g.c.h.i.b.o.ForwardingBigQueryFileOutputFormat@76] - Delegating functionality to 'AvroOutputFormat'.
2021-10-01 17:43:52,067 - DEBUG [SparkListenerBus:i.c.c.a.r.s.SparkTransactionHandler@144] - Spark job ended: JobTransaction{jobId=0, stageIds=[0], transaction=Transaction{readPointer: 1633110226293000000, transactionId: 1633110226328000000, writePointer: 1633110226328000000, invalids: [], inProgress: [1633110226294000000], firstShortInProgress: 1633110226294000000, type: LONG, checkpointWritePointers: [], visibilityLevel: SNAPSHOT}}
2021-10-01 17:43:55,199 - DEBUG [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:i.c.p.g.b.s.BigQueryOutputFormat@196] - Allow schema relaxation: 'true'
2021-10-01 17:43:55,199 - DEBUG [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:i.c.p.g.b.s.BigQueryOutputFormat@198] - Create Partitioned Table type: 'TIME'
2021-10-01 17:43:55,200 - DEBUG [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:i.c.p.g.b.s.BigQueryOutputFormat@201] - Partition Field: 'null'
2021-10-01 17:43:55,200 - DEBUG [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:i.c.p.g.b.s.BigQueryOutputFormat@203] - Require partition filter: 'false'
2021-10-01 17:43:55,201 - DEBUG [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:i.c.p.g.b.s.BigQueryOutputFormat@219] - Partition filter: 'null'
2021-10-01 17:43:55,201 - INFO  [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:i.c.p.g.b.s.BigQueryOutputFormat@272] - Importing into table 'cdf-athena:test_automation.DemoCheck1' from 1 paths; path[0] is 'gs://13d3737e-a176-4b78-beab-5f18dcca2353/13d3737e-a176-4b78-beab-5f18dcca2353/input/DemoCheck1-13d3737e-a176-4b78-beab-5f18dcca2353/part-r-00000.avro'; awaitCompletion: true
2021-10-01 17:43:55,205 - INFO  [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:i.c.p.g.b.s.BigQueryOutputFormat@375] - Using schema 'GenericData{classInfo=[fields], {fields=[{"mode":"REQUIRED","name":"name","type":"STRING"}, {"mode":"REQUIRED","name":"age","type":"INTEGER"}]}}' for the load job config.
2021-10-01 17:43:55,921 - DEBUG [spark-submitter-phase-1-1d3581a5-22df-11ec-af7e-eee09c71e460:i.c.c.a.r.s.SparkRuntimeEnv$@347] - Shutting down Server and ThreadPool used by Spark org.apache.spark.SparkContext@2c5a4a66
2021-10-01 17:43:55,926 - INFO  [SparkDriverHttpService STOPPING:i.c.h.NettyHttpService@258] - Stopping HTTP Service phase-1-http-service
2021-10-01 17:43:55,941 - DEBUG [SparkDriverHttpService STOPPING:i.c.h.NettyHttpService@276] - Stopped HTTP Service phase-1-http-service on address localhost/127.0.0.1:43627
2021-10-01 17:43:56,849 - DEBUG [SparkRunnerphase-1:i.c.p.g.b.u.BigQueryUtil@713] - Deleted temporary directory 'gs://13d3737e-a176-4b78-beab-5f18dcca2353'
2021-10-01 17:43:56,851 - DEBUG [SparkRunnerphase-1:i.c.c.a.r.s.SparkRuntimeService@896] - Running Spark shutdown hook org.apache.spark.util.SparkShutdownHookManager$$anon$2@1104ad62
2021-10-01 17:43:56,860 - DEBUG [SparkRunnerphase-1:i.c.c.a.r.s.SparkRuntimeService@376] - Spark program completed: SparkRuntimeContext{id=program:default.TestPipelinefbe63d25-c448-4ed4-baa5-22cafd7d3b66.-SNAPSHOT.spark.phase-1, runId=1d3581a5-22df-11ec-af7e-eee09c71e460}
2021-10-01 17:43:56,912 - ERROR [SparkRunnerphase-1:i.c.c.i.a.r.ProgramControllerServiceAdapter@92] - Spark Program 'phase-1' failed.
java.util.concurrent.ExecutionException: java.io.IOException: Failed to import GCS into BigQuery. 
 at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:294) ~[com.google.guava.guava-13.0.1.jar:na]
 at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:281) ~[com.google.guava.guava-13.0.1.jar:na]
 at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) ~[com.google.guava.guava-13.0.1.jar:na]
 at io.cdap.cdap.app.runtime.spark.SparkRuntimeService.run(SparkRuntimeService.java:346) ~[io.cdap.cdap.cdap-spark-core2_2.11-6.6.0-SNAPSHOT.jar:na]
 at com.google.common.util.concurrent.AbstractExecutionThreadService$1$1.run(AbstractExecutionThreadService.java:52) ~[com.google.guava.guava-13.0.1.jar:na]
 at io.cdap.cdap.app.runtime.spark.SparkRuntimeService$5$1.run(SparkRuntimeService.java:404) [io.cdap.cdap.cdap-spark-core2_2.11-6.6.0-SNAPSHOT.jar:na]
 at java.lang.Thread.run(Thread.java:748) [na:1.8.0_292]
Caused by: java.io.IOException: Failed to import GCS into BigQuery. 
 at io.cdap.plugin.gcp.bigquery.sink.BigQueryOutputFormat$BigQueryOutputCommitter.commitJob(BigQueryOutputFormat.java:227) ~[na:na]
 at io.cdap.cdap.etl.spark.io.TrackingOutputCommitter.commitJob(TrackingOutputCommitter.java:51) ~[na:na]
 at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1159) ~[na:na]
 at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085) ~[na:na]
 at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085) ~[na:na]
 at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[na:na]
 at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[na:na]
 at org.apache.spark.rdd.RDD.withScope(RDD.scala:362) ~[na:na]
 at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1085) ~[na:na]
 at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopDataset(JavaPairRDD.scala:831) ~[na:na]
 at io.cdap.cdap.etl.spark.batch.RDDUtils.saveHadoopDataset(RDDUtils.java:58) ~[na:na]
 at io.cdap.cdap.etl.spark.batch.RDDUtils.saveUsingOutputFormat(RDDUtils.java:47) ~[na:na]
 at io.cdap.cdap.etl.spark.batch.SparkBatchSinkFactory.writeFromRDD(SparkBatchSinkFactory.java:175) ~[na:na]
 at io.cdap.cdap.etl.spark.batch.BaseRDDCollection$1.run(BaseRDDCollection.java:239) ~[na:na]
 at io.cdap.cdap.etl.spark.SparkPipelineRunner.runPipeline(SparkPipelineRunner.java:383) ~[na:na]
 at io.cdap.cdap.etl.spark.batch.BatchSparkPipelineDriver.run(BatchSparkPipelineDriver.java:218) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.SparkTransactional$2.run(SparkTransactional.java:236) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.SparkTransactional.execute(SparkTransactional.java:208) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.SparkTransactional.execute(SparkTransactional.java:138) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.AbstractSparkExecutionContext.execute(AbstractSparkExecutionContext.scala:227) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.SerializableSparkExecutionContext.execute(SerializableSparkExecutionContext.scala:61) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.DefaultJavaSparkExecutionContext.execute(DefaultJavaSparkExecutionContext.scala:89) ~[na:na]
 at io.cdap.cdap.api.Transactionals.execute(Transactionals.java:63) ~[na:na]
 at io.cdap.cdap.etl.spark.batch.BatchSparkPipelineDriver.run(BatchSparkPipelineDriver.java:155) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.SparkMainWrapper$.main(SparkMainWrapper.scala:87) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.SparkMainWrapper.main(SparkMainWrapper.scala) ~[na:na]
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_292]
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_292]
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_292]
 at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_292]
 at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:744) ~[na:na]
 at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187) ~[na:na]
 at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212) ~[na:na]
 at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126) ~[na:na]
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.submit.AbstractSparkSubmitter.submit(AbstractSparkSubmitter.java:170) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.submit.AbstractSparkSubmitter.access$000(AbstractSparkSubmitter.java:54) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.submit.AbstractSparkSubmitter$4.run(AbstractSparkSubmitter.java:109) ~[na:na]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_292]
 at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_292]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_292]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_292]
 ... 1 common frames omitted
Caused by: java.io.IOException: Unhandled exception trying to insert job 'GenericData{classInfo=[configuration, etag, id, jobReference, kind, selfLink, statistics, status, user_email], {configuration=GenericData{classInfo=[copy, dryRun, extract, jobTimeoutMs, jobType, labels, load, query], {load=GenericData{classInfo=[allowJaggedRows, allowQuotedNewlines, autodetect, clustering, createDisposition, decimalTargetTypes, destinationEncryptionConfiguration, destinationTable, destinationTableProperties, encoding, fieldDelimiter, hivePartitioningOptions, ignoreUnknownValues, jsonExtension, maxBadRecords, nullMarker, parquetOptions, projectionFields, quote, rangePartitioning, schema, schemaInline, schemaInlineFormat, schemaUpdateOptions, skipLeadingRows, sourceFormat, sourceUris, timePartitioning, useAvroLogicalTypes, writeDisposition], {destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=test_automation, projectId=cdf-athena, tableId=DemoCheck1}}, schema=GenericData{classInfo=[fields], {fields=[{"mode":"REQUIRED","name":"name","type":"STRING"}, {"mode":"REQUIRED","name":"age","type":"INTEGER"}]}}, sourceFormat=AVRO, sourceUris=[gs://13d3737e-a176-4b78-beab-5f18dcca2353/13d3737e-a176-4b78-beab-5f18dcca2353/input/DemoCheck1-13d3737e-a176-4b78-beab-5f18dcca2353/part-r-00000.avro], timePartitioning=GenericData{classInfo=[expirationMs, field, requirePartitionFilter, type], {requirePartitionFilter=false, type=DAY}}, useAvroLogicalTypes=true, writeDisposition=WRITE_TRUNCATE}}}}, jobReference=GenericData{classInfo=[jobId, location, projectId], {jobId=a3fd1d12-9280-48ed-856d-ba603a29d8c2, location=US, projectId=cdf-athena}}}}'
 at com.google.cloud.hadoop.io.bigquery.BigQueryHelper.insertJobOrFetchDuplicate(BigQueryHelper.java:368) ~[na:na]
 at io.cdap.plugin.gcp.bigquery.sink.BigQueryOutputFormat$BigQueryOutputCommitter.triggerBigqueryJob(BigQueryOutputFormat.java:424) ~[na:na]
 at io.cdap.plugin.gcp.bigquery.sink.BigQueryOutputFormat$BigQueryOutputCommitter.importFromGcs(BigQueryOutputFormat.java:392) ~[na:na]
 at io.cdap.plugin.gcp.bigquery.sink.BigQueryOutputFormat$BigQueryOutputCommitter.commitJob(BigQueryOutputFormat.java:223) ~[na:na]
 ... 42 common frames omitted
Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden
POST https://bigquery.googleapis.com/bigquery/v2/projects/cdf-athena/jobs
{
  "code" : 403,
  "errors" : [ {
    "domain" : "global",
    "message" : "Access Denied: Project cdf-athena: User does not have bigquery.jobs.create permission in project cdf-athena.",
    "reason" : "accessDenied"
  } ],
  "message" : "Access Denied: Project cdf-athena: User does not have bigquery.jobs.create permission in project cdf-athena.",
  "status" : "PERMISSION_DENIED"
}
 at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146) ~[na:na]
 at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118) ~[na:na]
 at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37) ~[na:na]
 at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428) ~[na:na]
 at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111) ~[na:na]
 at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514) ~[na:na]
 at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455) ~[na:na]
 at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565) ~[na:na]
 at com.google.cloud.hadoop.io.bigquery.BigQueryHelper.insertJobOrFetchDuplicate(BigQueryHelper.java:352) ~[na:na]
 ... 45 common frames omitted
2021-10-01 17:43:56,961 - ERROR [SparkRunnerphase-1:i.c.c.i.a.r.ProgramControllerServiceAdapter@93] - Spark program 'phase-1' failed with error: 403 Forbidden
POST https://bigquery.googleapis.com/bigquery/v2/projects/cdf-athena/jobs
{
  "code" : 403,
  "errors" : [ {
    "domain" : "global",
    "message" : "Access Denied: Project cdf-athena: User does not have bigquery.jobs.create permission in project cdf-athena.",
    "reason" : "accessDenied"
  } ],
  "message" : "Access Denied: Project cdf-athena: User does not have bigquery.jobs.create permission in project cdf-athena.",
  "status" : "PERMISSION_DENIED"
}. Please check the system logs for more details.
com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden
POST https://bigquery.googleapis.com/bigquery/v2/projects/cdf-athena/jobs
{
  "code" : 403,
  "errors" : [ {
    "domain" : "global",
    "message" : "Access Denied: Project cdf-athena: User does not have bigquery.jobs.create permission in project cdf-athena.",
    "reason" : "accessDenied"
  } ],
  "message" : "Access Denied: Project cdf-athena: User does not have bigquery.jobs.create permission in project cdf-athena.",
  "status" : "PERMISSION_DENIED"
}
 at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146) ~[na:na]
 at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118) ~[na:na]
 at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37) ~[na:na]
 at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428) ~[na:na]
 at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111) ~[na:na]
 at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514) ~[na:na]
 at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455) ~[na:na]
 at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565) ~[na:na]
 at com.google.cloud.hadoop.io.bigquery.BigQueryHelper.insertJobOrFetchDuplicate(BigQueryHelper.java:352) ~[na:na]
 at io.cdap.plugin.gcp.bigquery.sink.BigQueryOutputFormat$BigQueryOutputCommitter.triggerBigqueryJob(BigQueryOutputFormat.java:424) ~[na:na]
 at io.cdap.plugin.gcp.bigquery.sink.BigQueryOutputFormat$BigQueryOutputCommitter.importFromGcs(BigQueryOutputFormat.java:392) ~[na:na]
 at io.cdap.plugin.gcp.bigquery.sink.BigQueryOutputFormat$BigQueryOutputCommitter.commitJob(BigQueryOutputFormat.java:223) ~[na:na]
 at io.cdap.cdap.etl.spark.io.TrackingOutputCommitter.commitJob(TrackingOutputCommitter.java:51) ~[na:na]
 at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1159) ~[na:na]
 at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085) ~[na:na]
 at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085) ~[na:na]
 at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[na:na]
 at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[na:na]
 at org.apache.spark.rdd.RDD.withScope(RDD.scala:362) ~[na:na]
 at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1085) ~[na:na]
 at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopDataset(JavaPairRDD.scala:831) ~[na:na]
 at io.cdap.cdap.etl.spark.batch.RDDUtils.saveHadoopDataset(RDDUtils.java:58) ~[na:na]
 at io.cdap.cdap.etl.spark.batch.RDDUtils.saveUsingOutputFormat(RDDUtils.java:47) ~[na:na]
 at io.cdap.cdap.etl.spark.batch.SparkBatchSinkFactory.writeFromRDD(SparkBatchSinkFactory.java:175) ~[na:na]
 at io.cdap.cdap.etl.spark.batch.BaseRDDCollection$1.run(BaseRDDCollection.java:239) ~[na:na]
 at io.cdap.cdap.etl.spark.SparkPipelineRunner.runPipeline(SparkPipelineRunner.java:383) ~[na:na]
 at io.cdap.cdap.etl.spark.batch.BatchSparkPipelineDriver.run(BatchSparkPipelineDriver.java:218) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.SparkTransactional$2.run(SparkTransactional.java:236) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.SparkTransactional.execute(SparkTransactional.java:208) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.SparkTransactional.execute(SparkTransactional.java:138) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.AbstractSparkExecutionContext.execute(AbstractSparkExecutionContext.scala:227) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.SerializableSparkExecutionContext.execute(SerializableSparkExecutionContext.scala:61) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.DefaultJavaSparkExecutionContext.execute(DefaultJavaSparkExecutionContext.scala:89) ~[na:na]
 at io.cdap.cdap.api.Transactionals.execute(Transactionals.java:63) ~[na:na]
 at io.cdap.cdap.etl.spark.batch.BatchSparkPipelineDriver.run(BatchSparkPipelineDriver.java:155) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.SparkMainWrapper$.main(SparkMainWrapper.scala:87) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.SparkMainWrapper.main(SparkMainWrapper.scala) ~[na:na]
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_292]
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_292]
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_292]
 at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_292]
 at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:744) ~[na:na]
 at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187) ~[na:na]
 at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212) ~[na:na]
 at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126) ~[na:na]
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.submit.AbstractSparkSubmitter.submit(AbstractSparkSubmitter.java:170) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.submit.AbstractSparkSubmitter.access$000(AbstractSparkSubmitter.java:54) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.submit.AbstractSparkSubmitter$4.run(AbstractSparkSubmitter.java:109) ~[na:na]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_292]
 at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_292]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_292]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_292]
 at java.lang.Thread.run(Thread.java:748) [na:1.8.0_292]
2021-10-01 17:43:56,975 - ERROR [WorkflowDriver:i.c.c.d.SmartWorkflow@561] - Pipeline 'TestPipelinefbe63d25-c448-4ed4-baa5-22cafd7d3b66' failed.
2021-10-01 17:43:57,020 - ERROR [WorkflowDriver:i.c.c.i.a.r.w.WorkflowProgramController@89] - Workflow service 'workflow.default.TestPipelinefbe63d25-c448-4ed4-baa5-22cafd7d3b66.DataPipelineWorkflow.1aeb91a4-22df-11ec-9a21-eee09c71e460' failed.
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.IOException: Failed to import GCS into BigQuery. 
 at java.util.concurrent.FutureTask.report(FutureTask.java:122) ~[na:1.8.0_292]
 at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[na:1.8.0_292]
 at io.cdap.cdap.internal.app.runtime.workflow.WorkflowDriver.executeAction(WorkflowDriver.java:344) ~[na:na]
 at io.cdap.cdap.internal.app.runtime.workflow.WorkflowDriver.executeNode(WorkflowDriver.java:475) ~[na:na]
 at io.cdap.cdap.internal.app.runtime.workflow.WorkflowDriver.executeAll(WorkflowDriver.java:641) ~[na:na]
 at io.cdap.cdap.internal.app.runtime.workflow.WorkflowDriver.run(WorkflowDriver.java:626) ~[na:na]
 at com.google.common.util.concurrent.AbstractExecutionThreadService$1$1.run(AbstractExecutionThreadService.java:52) ~[com.google.guava.guava-13.0.1.jar:na]
 at java.lang.Thread.run(Thread.java:748) [na:1.8.0_292]
Caused by: java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.IOException: Failed to import GCS into BigQuery. 
 at com.google.common.base.Throwables.propagate(Throwables.java:160) ~[com.google.guava.guava-13.0.1.jar:na]
 at io.cdap.cdap.internal.app.runtime.workflow.DefaultProgramWorkflowRunner$1.run(DefaultProgramWorkflowRunner.java:143) ~[na:na]
 at io.cdap.cdap.internal.app.runtime.workflow.WorkflowDriver$1.call(WorkflowDriver.java:338) ~[na:na]
 at io.cdap.cdap.internal.app.runtime.workflow.WorkflowDriver$1.call(WorkflowDriver.java:322) ~[na:na]
 at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_292]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_292]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_292]
 ... 1 common frames omitted
Caused by: java.util.concurrent.ExecutionException: java.io.IOException: Failed to import GCS into BigQuery. 
 at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:294) ~[com.google.guava.guava-13.0.1.jar:na]
 at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:281) ~[com.google.guava.guava-13.0.1.jar:na]
 at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) ~[com.google.guava.guava-13.0.1.jar:na]
 at io.cdap.cdap.app.runtime.spark.SparkRuntimeService.run(SparkRuntimeService.java:346) ~[na:na]
 at com.google.common.util.concurrent.AbstractExecutionThreadService$1$1.run(AbstractExecutionThreadService.java:52) ~[com.google.guava.guava-13.0.1.jar:na]
 at io.cdap.cdap.app.runtime.spark.SparkRuntimeService$5$1.run(SparkRuntimeService.java:404) ~[na:na]
 ... 1 common frames omitted
Caused by: java.io.IOException: Failed to import GCS into BigQuery. 
 at io.cdap.plugin.gcp.bigquery.sink.BigQueryOutputFormat$BigQueryOutputCommitter.commitJob(BigQueryOutputFormat.java:227) ~[na:na]
 at io.cdap.cdap.etl.spark.io.TrackingOutputCommitter.commitJob(TrackingOutputCommitter.java:51) ~[na:na]
 at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1159) ~[na:na]
 at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085) ~[na:na]
 at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085) ~[na:na]
 at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) ~[na:na]
 at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) ~[na:na]
 at org.apache.spark.rdd.RDD.withScope(RDD.scala:362) ~[na:na]
 at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1085) ~[na:na]
 at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopDataset(JavaPairRDD.scala:831) ~[na:na]
 at io.cdap.cdap.etl.spark.batch.RDDUtils.saveHadoopDataset(RDDUtils.java:58) ~[na:na]
 at io.cdap.cdap.etl.spark.batch.RDDUtils.saveUsingOutputFormat(RDDUtils.java:47) ~[na:na]
 at io.cdap.cdap.etl.spark.batch.SparkBatchSinkFactory.writeFromRDD(SparkBatchSinkFactory.java:175) ~[na:na]
 at io.cdap.cdap.etl.spark.batch.BaseRDDCollection$1.run(BaseRDDCollection.java:239) ~[na:na]
 at io.cdap.cdap.etl.spark.SparkPipelineRunner.runPipeline(SparkPipelineRunner.java:383) ~[na:na]
 at io.cdap.cdap.etl.spark.batch.BatchSparkPipelineDriver.run(BatchSparkPipelineDriver.java:218) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.SparkTransactional$2.run(SparkTransactional.java:236) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.SparkTransactional.execute(SparkTransactional.java:208) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.SparkTransactional.execute(SparkTransactional.java:138) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.AbstractSparkExecutionContext.execute(AbstractSparkExecutionContext.scala:227) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.SerializableSparkExecutionContext.execute(SerializableSparkExecutionContext.scala:61) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.DefaultJavaSparkExecutionContext.execute(DefaultJavaSparkExecutionContext.scala:89) ~[na:na]
 at io.cdap.cdap.api.Transactionals.execute(Transactionals.java:63) ~[na:na]
 at io.cdap.cdap.etl.spark.batch.BatchSparkPipelineDriver.run(BatchSparkPipelineDriver.java:155) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.SparkMainWrapper$.main(SparkMainWrapper.scala:87) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.SparkMainWrapper.main(SparkMainWrapper.scala) ~[na:na]
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_292]
 at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_292]
 at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_292]
 at java.lang.reflect.Method.invoke(Method.java:498) ~[na:1.8.0_292]
 at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:744) ~[na:na]
 at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187) ~[na:na]
 at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212) ~[na:na]
 at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126) ~[na:na]
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.submit.AbstractSparkSubmitter.submit(AbstractSparkSubmitter.java:170) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.submit.AbstractSparkSubmitter.access$000(AbstractSparkSubmitter.java:54) ~[na:na]
 at io.cdap.cdap.app.runtime.spark.submit.AbstractSparkSubmitter$4.run(AbstractSparkSubmitter.java:109) ~[na:na]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_292]
 at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_292]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_292]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[na:1.8.0_292]
 ... 1 common frames omitted
Caused by: java.io.IOException: Unhandled exception trying to insert job 'GenericData{classInfo=[configuration, etag, id, jobReference, kind, selfLink, statistics, status, user_email], {configuration=GenericData{classInfo=[copy, dryRun, extract, jobTimeoutMs, jobType, labels, load, query], {load=GenericData{classInfo=[allowJaggedRows, allowQuotedNewlines, autodetect, clustering, createDisposition, decimalTargetTypes, destinationEncryptionConfiguration, destinationTable, destinationTableProperties, encoding, fieldDelimiter, hivePartitioningOptions, ignoreUnknownValues, jsonExtension, maxBadRecords, nullMarker, parquetOptions, projectionFields, quote, rangePartitioning, schema, schemaInline, schemaInlineFormat, schemaUpdateOptions, skipLeadingRows, sourceFormat, sourceUris, timePartitioning, useAvroLogicalTypes, writeDisposition], {destinationTable=GenericData{classInfo=[datasetId, projectId, tableId], {datasetId=test_automation, projectId=cdf-athena, tableId=DemoCheck1}}, schema=GenericData{classInfo=[fields], {fields=[{"mode":"REQUIRED","name":"name","type":"STRING"}, {"mode":"REQUIRED","name":"age","type":"INTEGER"}]}}, sourceFormat=AVRO, sourceUris=[gs://13d3737e-a176-4b78-beab-5f18dcca2353/13d3737e-a176-4b78-beab-5f18dcca2353/input/DemoCheck1-13d3737e-a176-4b78-beab-5f18dcca2353/part-r-00000.avro], timePartitioning=GenericData{classInfo=[expirationMs, field, requirePartitionFilter, type], {requirePartitionFilter=false, type=DAY}}, useAvroLogicalTypes=true, writeDisposition=WRITE_TRUNCATE}}}}, jobReference=GenericData{classInfo=[jobId, location, projectId], {jobId=a3fd1d12-9280-48ed-856d-ba603a29d8c2, location=US, projectId=cdf-athena}}}}'
 at com.google.cloud.hadoop.io.bigquery.BigQueryHelper.insertJobOrFetchDuplicate(BigQueryHelper.java:368) ~[na:na]
 at io.cdap.plugin.gcp.bigquery.sink.BigQueryOutputFormat$BigQueryOutputCommitter.triggerBigqueryJob(BigQueryOutputFormat.java:424) ~[na:na]
 at io.cdap.plugin.gcp.bigquery.sink.BigQueryOutputFormat$BigQueryOutputCommitter.importFromGcs(BigQueryOutputFormat.java:392) ~[na:na]
 at io.cdap.plugin.gcp.bigquery.sink.BigQueryOutputFormat$BigQueryOutputCommitter.commitJob(BigQueryOutputFormat.java:223) ~[na:na]
 ... 42 common frames omitted
Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden
POST https://bigquery.googleapis.com/bigquery/v2/projects/cdf-athena/jobs
{
  "code" : 403,
  "errors" : [ {
    "domain" : "global",
    "message" : "Access Denied: Project cdf-athena: User does not have bigquery.jobs.create permission in project cdf-athena.",
    "reason" : "accessDenied"
  } ],
  "message" : "Access Denied: Project cdf-athena: User does not have bigquery.jobs.create permission in project cdf-athena.",
  "status" : "PERMISSION_DENIED"
}
 at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146) ~[na:na]
 at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118) ~[na:na]
 at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37) ~[na:na]
 at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:428) ~[na:na]
 at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111) ~[na:na]
 at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:514) ~[na:na]
 at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:455) ~[na:na]
 at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:565) ~[na:na]
 at com.google.cloud.hadoop.io.bigquery.BigQueryHelper.insertJobOrFetchDuplicate(BigQueryHelper.java:352) ~[na:na]
 ... 45 common frames omitted
2021-10-01 17:43:57,030 - DEBUG [pcontroller-program:default.TestPipelinefbe63d25-c448-4ed4-baa5-22cafd7d3b66.-SNAPSHOT.workflow.DataPipelineWorkflow-1aeb91a4-22df-11ec-9a21-eee09c71e460-3:i.c.c.a.r.AbstractProgramRuntimeService@564] - RuntimeInfo removed: program_run:default.TestPipelinefbe63d25-c448-4ed4-baa5-22cafd7d3b66.-SNAPSHOT.workflow.DataPipelineWorkflow.1aeb91a4-22df-11ec-9a21-eee09c71e460
2021-10-01 17:43:58,922 - DEBUG [provisioning-task-3:i.c.c.i.p.t.ProvisioningTask@116] - Completed DEPROVISION task for program run program_run:default.TestPipelinefbe63d25-c448-4ed4-baa5-22cafd7d3b66.-SNAPSHOT.workflow.DataPipelineWorkflow.1aeb91a4-22df-11ec-9a21-eee09c71e460.</div>
		</li>
		<li test-id='27' class='node * pass' status='pass'>
			<div class="step-name" title=""><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>AfterActions.tearDown(Scenario)</div>
			<div class="node-step"></div>
		</li>
		<li test-id='28' class='node then fail' status='fail'>
			<div class="step-name" title="GCSBasicDemo.verifyThePipelineStatusIs(String)"><span class='status fail' title='fail'><i class='material-icons'>cancel</i></span>Then Verify the pipeline status is "Succeeded"</div>
			<textarea disabled class="code-block">java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:87)
	at org.junit.Assert.assertTrue(Assert.java:42)
	at org.junit.Assert.assertTrue(Assert.java:53)
	at stepsDesign.GCSBasicDemo.verifyThePipelineStatusIs(GCSBasicDemo.java:140)
	at ✽.Verify the pipeline status is "Succeeded"(file:src/test/resources/Test/DemoGcs.feature:17)
</textarea>
		</li>
		<li test-id='29' class='node * pass' status='pass'>
			<div class="step-name" title=""><span class='status pass' title='pass'><i class='material-icons'>check_circle</i></span>AfterActions.tearDown(Scenario)</div>
			<div class="node-step"></div>
		</li>
		<li test-id='30' class='node then skip' status='skip'>
			<div class="step-name" title="GCSBasicDemo.validateSuccessMessageIsDisplayed()"><span class='status skip' title='skip'><i class='material-icons'>redo</i></span>Then validate successMessage is displayed</div>
			<div class="node-step">Step skipped</div>
		</li>
		<li test-id='31' class='node * skip' status='skip'>
			<div class="step-name" title=""><span class='status skip' title='skip'><i class='material-icons'>redo</i></span>AfterActions.tearDown(Scenario)</div>
			<div class="node-step">Step skipped</div>
		</li>
		<li test-id='32' class='node then skip' status='skip'>
			<div class="step-name" title="GCSBasicDemo.getCountOfNoOfRecordsTransferredToBigQueryIn(String)"><span class='status skip' title='skip'><i class='material-icons'>redo</i></span>Then Get Count of no of records transferred to BigQuery in "tableDemo"</div>
			<div class="node-step">Step skipped</div>
		</li>
		<li test-id='33' class='node * skip' status='skip'>
			<div class="step-name" title=""><span class='status skip' title='skip'><i class='material-icons'>redo</i></span>AfterActions.tearDown(Scenario)</div>
			<div class="node-step">Step skipped</div>
		</li>
		<li test-id='34' class='node then skip' status='skip'>
			<div class="step-name" title="GCSBasicDemo.deleteTheTable(String)"><span class='status skip' title='skip'><i class='material-icons'>redo</i></span>Then Delete the table "tableDemo"</div>
			<div class="node-step">Step skipped</div>
		</li>
		<li test-id='35' class='node * skip' status='skip'>
			<div class="step-name" title=""><span class='status skip' title='skip'><i class='material-icons'>redo</i></span>AfterActions.tearDown(Scenario)</div>
			<div class="node-step">Step skipped</div>
		</li>
	</ul>
</div>
					</div>
				</li>
			</ul>
		</div>
	</div>
	<!-- subview left -->
	<div class='subview-right left'>
		<div class='view-summary'>
			<div id='step-filters' class="right sr-filters">
				<a class="btn-floating waves-effect waves-light green" status="pass" alt="pass" title="pass"><i class="material-icons">check_circle</i></a>
				<a class="btn-floating waves-effect waves-light red" status="fail" alt="fail" title="fail"><i class="material-icons">cancel</i></a>
				<a class="btn-floating waves-effect waves-light red darken-4" status="fatal" alt="fatal" title="fatal"><i class="material-icons">cancel</i></a>
				<a class="btn-floating waves-effect waves-light pink text-lighten-1" status="error" alt="error" title="error"><i class="material-icons">error</i></a>
				<a class="btn-floating waves-effect waves-light orange" alt="warning" status="warning" title="warning"><i class="material-icons">warning</i></a>
				<a class="btn-floating waves-effect waves-light teal" status="skip" alt="skip" title="skip"><i class="material-icons">redo</i></a>
				<a class="btn-floating waves-effect waves-light grey" status="clear" alt="Clear filters" title="Clear filters"><i class="material-icons">clear</i></a>
			</div>
			<h5 class='test-name'></h5>
		</div>
	</div>
	<!-- subview right -->
</div>
<!-- test view --><div id='category-view' class='view hide'>
	<section id='controls'>
		<div class='controls grey lighten-4'>
			<!-- search -->
			<div class='chip transparent' alt='Search Tests' title='Search Tests'>
				<a href="#" class='search-div'>
				<i class='material-icons'>search</i> Search
				</a>
				<div class='input-field left hide'>
					<input id='search-tests' type='text' class='validate browser-default' placeholder='Search Tests...'>
				</div>
			</div>
			<!-- search -->
		</div>
	</section>
	<div class='subview-left left'>
		<div class='view-summary'>
			<ul id='category-collection' class='category-collection'>
				<li class='category displayed active'>
					<div class='category-heading'>
						<span class='category-name'>@TC-Demo-1</span>
						<span class='category-status right'>
						<span class='label fail'>1</span>
						</span>
					</div>
					<div class='category-content hide'>
						<div class='category-status-counts'>
							
							<span status="fail" class='label red lighten-1 white-text'>Failed: 1</span>
							
						</div>
						<div class='category-tests'>
							<table class='bordered table-results'>
								<thead>
									<tr>
										<th>Timestamp</th>
										<th>TestName</th>
										<th>Status</th>
									</tr>
								</thead>
								<tbody>
									<tr>
										<td>Oct 1, 2021 05:42:49 PM</td>
										<td class='linked' test-id='2'>User is able to Login and confirm data is getting transferred from GCS to BigQuery</td>
										<td><span class='test-status fail'>fail</span></td>
									</tr>
								</tbody>
							</table>
						</div>
					</div>
				</li>
			</ul>
		</div>
	</div>
	<div class='subview-right left'>
		<div class='view-summary'>
			<h5 class='category-name'></h5>
		</div>
	</div>
</div>
<!-- category view --><div id='exception-view' class='view hide'>
	<section id='controls'>
		<div class='controls grey lighten-4'>
			<!-- search -->
			<div class='chip transparent' alt='Search Tests' title='Search Tests'>
				<a href="#" class='search-div'>
				<i class='material-icons'>search</i> Search
				</a>
				<div class='input-field left hide'>
					<input id='search-tests' type='text' class='validate browser-default' placeholder='Search Tests...'>
				</div>
			</div>
			<!-- search -->
		</div>
	</section>
	<div class='subview-left left'>
		<div class='view-summary'>
			<ul id='exception-collection' class='exception-collection'>
				<li class='exception displayed active'>
					<div class='exception-heading'>
						<span class='exception-name'>java.lang.AssertionError</span>
						<span class='exception-count right'><span class='label red lighten-1 white-text'>1</span></span>
					</div>
					<div class='exception-content hide'>
						<div class='exception-tests'>
							<table class='bordered table-results'>
								<thead>
									<tr>
										<th>Timestamp</th>
										<th>TestName</th>
										<th>Status</th>
									</tr>
								</thead>
								<tbody>
									<tr>
										<td>Oct 1, 2021 05:44:27 PM</td>
										<td class='linked' test-id='28'>Then Verify the pipeline status is "Succeeded"</td>
										<td>
											<textarea disabled class="code-block">java.lang.AssertionError
	at org.junit.Assert.fail(Assert.java:87)
	at org.junit.Assert.assertTrue(Assert.java:42)
	at org.junit.Assert.assertTrue(Assert.java:53)
	at stepsDesign.GCSBasicDemo.verifyThePipelineStatusIs(GCSBasicDemo.java:140)
	at ✽.Verify the pipeline status is "Succeeded"(file:src/test/resources/Test/DemoGcs.feature:17)
</textarea>
										</td>
									</tr>
								</tbody>
							</table>
						</div>
					</div>
				</li>
			</ul>
		</div>
	</div>
	<div class='subview-right left'>
		<div class='view-summary'>
			<h5 class='exception-name'></h5>
		</div>
	</div>
</div>
<!-- exception view --><div id='dashboard-view' class='view hide'>
	<div class='card-panel transparent np-v'>
		<h5>Dashboard</h5>

		<div class='row'>
			<div class='col s2'>
				<div class='card-panel r'>
					Features
					<div class='panel-lead'>1</div>
				</div>
			</div>
			<div class='col s2'>
				<div class='card-panel r'>
					Scenarios
					<div class='panel-lead'>1</div>
				</div>
			</div>
			<div class='col s2'>
				<div class='card-panel r'>
					Steps
					<div class='panel-lead'>32</div>
				</div>
			</div>
			<div class='col s2'>
				<div class='card-panel r'>
					Start
					<div class='panel-lead'>Oct 1, 2021 05:42:48 PM</div>
				</div>
			</div>
			<div class='col s2'>
				<div class='card-panel r'>
			 		End
			 		<div class='panel-lead'>Oct 1, 2021 05:45:18 PM</div>
				</div>
			</div>
			<div class='col s2'>
				<div class='card-panel r'>
					Time Taken
					<div class='panel-lead'>0h 2m 30s+53ms</div>
				</div>
			</div>
			<div class='col s6'>
				<div class='card-panel dashboard-categories'>
					<span class='right label cyan white-text'>Categories</span><p>&nbsp;</p>
					
					<table>
						<tr>
							<th>Name</th>
							<th>Passed</th>
							<th>Failed</th>
							<th>Skipped</th>
							<th>Passed %</th>
						</tr>
						<tr>
							<td>@TC-Demo-1</td>
							<td>0</td>
							<td>1</td>
							<td>0</td>
							<td>
									0%
							</td>
						</tr>
					</table>
				</div>
			</div>
		</div>
	</div>
</div>
<!-- dashboard view -->
<!-- testrunner-logs view -->		</div>
		<!-- container -->
		<script>
			var statusGroup = {
                parentCount: 1,
				passParent: 0,
				failParent: 1,
				fatalParent: 0,
				errorParent: 0,
				warningParent: 0,
				skipParent: 0,
				exceptionsParent: 1,
				childCount: 1,
				passChild: 0,
				failChild: 1,
				fatalChild: 0,
				errorChild: 0,
				warningChild: 0,
				skipChild: 0,
				infoChild: 0,
				debugChild: 0,
				exceptionsChild: 1,
				grandChildCount: 32,
				passGrandChild: 25,
				failGrandChild: 1,
				fatalGrandChild: 0,
				errorGrandChild: 0,
				warningGrandChild: 0,
				skipGrandChild: 6,
				infoGrandChild: 0,
				debugGrandChild: 0,
				exceptionsGrandChild: 7,
			};
		</script>
		<script>
			var timeline = {
			        "Demo1":148.42
			};
		</script>
	
		  <script src='http://cdn.jsdelivr.net/gh/extent-framework/extent-github-cdn@ff53917fbbdb5ef820abbbe4d199a6942dc771ff/v3html/js/extent.js' type='text/javascript'></script>
		<script type='text/javascript'>
			


$(document).ready(function() {
});

		
		</script>
	</body>
</html>